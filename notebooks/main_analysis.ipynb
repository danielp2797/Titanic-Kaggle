{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "## 0. [Imports, data loading and initial checking](#DL)\n",
    "## 1. [Features properties](#FP)\n",
    "## 2. [Features enginering](#FE)\n",
    "## 3. [Model hipothesis and discussion](#MH)\n",
    "## 4. [Model building](#MB)\n",
    "## 5. [Model selection](#MS)\n",
    "## 6. [Conclussions](#C)\n",
    "## 7. [Submission](#S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys \n",
    "sys.path.insert(0, '../src/visualization/') # add the path which contains visualize\n",
    "sys.path.insert(0, '../src/data/') # add the path which containing visualize\n",
    "sys.path.insert(0, '../src/features/') # add the path which contains visualize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import visualize as vsl  # own visualize.py file containing every visualization scripts\n",
    "import fill_nulls as fln  # own fill_nulls.py file containing scripts to fill nan\n",
    "import build_features as bf  # own build_features.py containing the script to generate features dataset \n",
    "from titanic_submission import submit_result  # own titanic_submission.py script to automatize submissions\n",
    "\n",
    "from scipy.stats import kstest, kurtosis, shapiro, skew, boxcox\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams[\"figure.figsize\"] = [9,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports, data loading and initial checking <a class=\"anchor\" id=\"DL\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/raw/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that Age, Embarked and Cabin contain null values, so, we will keep it in mind (in order to fill them) if we consider that some of these features is relevant after the properties overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Features properties <a class=\"anchor\" id=\"FP\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to study the features properties in order to two main aspects:\n",
    "\n",
    "1. Marginal Distribution to know how a feature acts marginally \n",
    "\n",
    "2. Joint distribution with Survived to know which features are significantly related with the survivors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology is basicly take an overview with a pie chart (in the case of cathegorical features) or an histogram (in the case of numerical values), then we will plot the joint distribution with a stacked bars diagram (in the case of cathegorical features) or an histogram hued by survivor (in the case of numerical features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 PassengerId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature seems to be a unique identifier for every passenger, but we do not need it because it matches with our dataframe indexes. It will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop('PassengerId', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Pclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_pie_chart(train_data['Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT DISTRIBUTION WITH SURVIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Pclass'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graphic we can see a relation between Survived and Pclass, as we can see, when Pclass decreases survivors proportion increases, so, it could be a feature in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_pie_chart(train_data['Pclass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT DISTRIBUTION WITH SURVIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Sex'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that females represent aprroximately the thitd part of the passage, but, they have a significant higher probability to survive than males, so, it could be a feature in the model due to the strong relation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(train_data['Age'])\n",
    "plt.title('Marginal distribution of Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above histogram we can conclude that a great majority of the passage is between 18 and 30 years approximately. It seems a distribution skewed to the right and it is not any well-known distribution. Specially, The normality assumption it is not justified as we can see in the Kolmogorov-Smirnov pvalue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(train_data['Age'].dropna(), 'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In spite of non normality, we can assume that the distribution is approximately symmetric as we can see in kurtosis and skew coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('kurtosis:', kurtosis(train_data['Age'].dropna()), 'skew:', skew(train_data['Age'].dropna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT DISTRIBUTION WITH SURVIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train_data, x='Age', hue='Survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice than childs have the highest chance to survive, and in the center of the distribution (young and middle-age persons) have approximately the same probability to survive or not. For the oldest people we can see that, in overall, nobody survived. \n",
    "\n",
    "That feature seems useful to predict survivors,so, we will include it in the model, but first, we have to implement a method to fill the null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 SibSp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_pie_chart(train_data['SibSp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT DISTRIBUTION WITH SURVIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['SibSp'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the probability of survive decreases when SibSp increases (in overall), this feature could be included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Parch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_pie_chart(train_data['Parch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT DISTRIBUTION WITH SURVIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Parch'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the joint distribution we can see a (slight) relation between the Parch and Survived, where Parch increases the probability of survive decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Comment*: In overall, that feature has the same relation with survived as SibSp, so, it could be a good idea merge both features in a unique feature to prevent correlation between features in the future model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Fare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train_data, x='Fare', bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT DISTRIBUTION WITH SURVIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train_data, x='Fare', hue='Survived', bins=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the probability of survive increases when Fare increases, so, it could be a feature in the model, but, probably, its correlated with Pclass, we have to take care if we include both features to prevent high correlations between model features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MARGINAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_pie_chart(train_data['Embarked'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINT DISTRIBUTION WITH SURVIVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Embarked'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a slight relation between the place where passenger embarked and the probability of survive, but it does not seem significant. We will test different models including or not this feature and we will se if it is relevant or not.\n",
    "\n",
    "In addition, it has null values as we can see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Embarked'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the small number of null values we will replace it with the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_embarked = train_data['Embarked'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data['Embarked'].isnull(), 'Embarked'] = mode_embarked\n",
    "train_data['Embarked'].isnull().sum()  # check if the null values are filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a simplistic approach, names do not seem a reason to survive a disaster, but, if we remind that we need to fill Age null values, definetly, the salutations before the surname can be very useful to locate persons in an age band. We will develop these ideas later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Cabin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable to think that cabins distribution could be a reason to survive in order to some natural factors as the distance to the nearest safeboats or the initial hole in the boat, so, we are going to check if this feature has a relation with survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Cabin'].fillna('Missing').apply(lambda x: x[:1]),\n",
    "                             train_data['Survived'].loc[train_data['Cabin'].index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the survive probability conditioned by the cabin first letter is different for every Cabin value, Cabins B, D and E have a higher proportion of suvivors than G, C, A. In addition every passengers in Cabin T died. It seems to be a relations and we could include that feature in the model.\n",
    "\n",
    "Note that we have filled the null values with the Cabin label 'Missing', it could be reasonable, as we can see above, beacause a great part of the passengers who traveled in that Cabin died, so, it is natural that their Cabin is unknown. \n",
    "\n",
    "Lets check too the relation between Cabin and some other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Cabin'].fillna('Missing').apply(lambda x: x[:1]),\n",
    "                             train_data['Pclass'].loc[train_data['Cabin'].index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that Cabins could be assigned by class, A, B, and C is assigned for the first class passengers, the others have mixed classes. So, we have to take care about correlation between both features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Features enginering <a class=\"anchor\" id=\"FE\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Name. A way to fill Age null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, as mentioned previously, we are going to extract passengers salutations with the goal to extract their titles, in addition, it could be a good stratification feature in order to infer an age band for passengers which would allow us to fill Age null values.\n",
    "\n",
    "Let's take a look of the names format in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format seems to be 'name, salutation. surname' so, the appropiate tool to get the salutations will be  a regex rule runned over every Name to keep the string inmediately before the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Title'] = train_data.Name.str.extract('([A-Za-z]+)\\.', expand=False) \n",
    "train_data['Title'].value_counts(normalize=True)  # overview of saludations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there common salutations such as Mr o Miss, but, we have find multiple salutations which are rare such as Countess or Jonkheer. Seems reasonable to make groups in order to the most common titles and mke and additional group containin rare titles:\n",
    "\n",
    "1. Mr and Mrs: man and women who are married\n",
    "2. Miss: women not married\n",
    "2. Master: an special title for minors. \n",
    "3. Rare: those which are not included above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Title'] = train_data['Title'].apply(lambda x: \n",
    "                          1 if x in ['Mr', 'Mrs'] else \n",
    "                          (2 if x == 'Miss' else \n",
    "                           (3 if x == 'Master' else 4)))\n",
    "\n",
    "group_means = []\n",
    "\n",
    "for age_group in train_data['Title'].unique().tolist():\n",
    "    \n",
    "    group_means.append(train_data[train_data['Title'].isin([age_group])]['Age'].mean())\n",
    "    \n",
    "means_dict = dict(zip(train_data['Title'].unique().tolist(), group_means)) # dict to use with replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as we can see our stratification seems reasonable, every group has a different age mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(means_dict, orient='index', columns=['mean']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can fill the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages_to_fill = train_data[train_data['Age'].isnull()]['Title'].replace(means_dict)\n",
    "ages_to_fill = ages_to_fill.apply(int) # transform age into int type\n",
    "train_data.loc[ages_to_fill.index, 'Age'] = ages_to_fill\n",
    "\n",
    "train_data['Age'].isnull().sum()  # we can check that there is no null values now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before leave this section it could a good idea check the joint distribution of title and Survived, as we said above, Age is related with Survived, so, it is reasonable that our new feature, Title, has inherited that relation, because salutations are based in the age, experience and knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Title'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the relation is inherited, and probably, it could be a good feature rather than simply Age, we will see in the model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Family features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we concluded above, SibSp and Parch can duplicate information about a passenger family status in the model, because they showed a similar relation with Survived. It could be a good idea to summarize both features in only one boolean feature that we will call Alone, which values are 1 if the passengers travles alone and 0 if the passenger travels with some family member/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Alone'] = (train_data['SibSp'] + train_data['Parch'] + 1).apply(lambda x: 0 if x!=1 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, now we check the relations with Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Alone'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see a slight relation between both variables, we will study the feature in the model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Age Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an intuition, I propose to create a boolean feature about the elder persons. It reasonable to think that persons who are very old, are more delicated than young persons, so, we will define the threshold to consider someone old in 65 years old, and we will check the relation with Survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Old'] = train_data['Age'].apply(lambda x: 1 if x>65 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsl.build_stacked_bars_chart(train_data['Old'], train_data['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems a strong relation, if some passenger is more than 65, its probability to survive is very low, in the other hand, persons who are not old have approximately the same probability of survive or not, and we could not reject the possibility that the hipothesis is true. We will check it in the model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model hipothesis and discussion <a class=\"anchor\" id=\"MH\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to analyse the features and the target variable in order to propose a set of models which can predict which passengers will survive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Target feature (to predict): boolean\n",
    "* Avaible features: boolean, cathegorical, numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, it is highly posible that some variables cumulate information from others, for example, seems reasonable that the highest fares are concentrated in the first Pclass, as we can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=train_data, x='Fare', hue='Pclass', bins=10, palette='summer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclussion from the above exploratory analysis, it seems clear that we have to solve a classification problem, so, the classical techniques used in this kind of problems are: Logistic Regression, Decission Trees and his variant Random Forest, Support Vector Classifier, linear classifiers (SVG), Naive Bayes Classifiers...\n",
    "\n",
    "At the beginning, we will try to fit the most simple and classical models such as Logistic Regression and Decission Trees. If it do not work as expected, we will add more complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model building <a class=\"anchor\" id=\"MB\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to prepare the data in order to apply models over a prepared features dataset, we will create all necessary dummy features and copy the original dataset columns in order to prevent undesiderable modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Features creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sex featrue\n",
    "Isfemale = pd.get_dummies(train_data['Sex'])['female']\n",
    "\n",
    "# extra features from Age, not very useful\n",
    "# Isold = train_data['Old'].copy()  \n",
    "\n",
    "# SibSp features \n",
    "Isalone = train_data['Alone'].copy()\n",
    "SibSp = train_data['SibSp'].copy()\n",
    "Parch = train_data['Parch'].copy()\n",
    "\n",
    "# class features\n",
    "C1 = pd.get_dummies(train_data['Pclass'])[1]\n",
    "C2 = pd.get_dummies(train_data['Pclass'])[2]\n",
    "Pclass = train_data['Pclass']\n",
    "\n",
    "# numeric features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Age = train_data['Age'].copy()\n",
    "Age = pd.Series(scaler.fit_transform(np.array(Age).reshape(-1,1))[:, 0])\n",
    "#Age (bins)\n",
    "Age_bins = pd.cut(train_data['Age'], bins=10, labels=range(10))\n",
    "\n",
    "\n",
    "Fare = train_data['Fare'].copy()\n",
    "Fare = pd.Series(scaler.fit_transform(np.array(Fare).reshape(-1,1))[:, 0])\n",
    "\n",
    "# Fare bins\n",
    "Fare_bins = pd.cut(train_data['Fare'], bins=10, labels=range(10))\n",
    "\n",
    "# embarked features\n",
    "Isq = pd.get_dummies(train_data['Embarked'])['Q']\n",
    "Iss = pd.get_dummies(train_data['Embarked'])['S']\n",
    "\n",
    "# title features\n",
    "IsMr = train_data['Name'].apply(lambda x: 1 if 'Mr' in x else 0)\n",
    "IsMrs = train_data['Name'].apply(lambda x: 1 if 'Mrs' in x else 0)\n",
    "Isminor = pd.get_dummies(train_data['Title'])[3]\n",
    "Ismiss = pd.get_dummies(train_data['Title'])[2]\n",
    "Israre = pd.get_dummies(train_data['Title'])[4]\n",
    "\n",
    "#Cabin features\n",
    "Cabin = pd.get_dummies(train_data['Cabin'].fillna('Missing').apply(lambda x: x[:1]))\n",
    "Cabin['M'] = Cabin['M'] + Cabin['T']  # add T Cabin to M\n",
    "Cabin.drop('T', axis=1, inplace=True)  # remove Cabin T\n",
    "\n",
    "X = pd.concat([Isfemale, Isalone, C1,\n",
    "           C2, Age, Fare, Isq, Iss, IsMr, IsMrs,\n",
    "           Isminor, Ismiss, Israre, SibSp, Parch,\n",
    "               Pclass, Age_bins, Fare_bins, Cabin], axis=1, ignore_index=True)\n",
    "\n",
    "feature_names = ['Isfemale', 'Isalone', 'C1',\n",
    "           'C2', 'Age', 'Fare', 'Isq', 'Iss', 'Ismr', 'Ismrs',\n",
    "           'Isminor', 'Ismiss', 'Israre', 'SibSp', 'Parch', 'Pclass',\n",
    "                 'Age_bins', 'Fare_bins', 'CabinA','CabinB',\n",
    "                 'CabinC', 'CabinD', 'CabinE',\t'CabinF', 'CabinG', 'CabinM']\n",
    "\n",
    "X.columns = feature_names\n",
    "\n",
    "y = train_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Running basic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split model data in train and test samples, and fit, as we said, a Logistic Regression model and a Random Forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISSION TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_decission_tree(features):\n",
    "    \n",
    "    DT = DecisionTreeClassifier(criterion='entropy', max_depth=3, min_samples_leaf=13,\n",
    "                                min_samples_split=4)\n",
    "\n",
    "    DT.fit(X_train[features], y_train)\n",
    "\n",
    "    scores = cross_val_score(DT, X_train, y_train, cv=10)\n",
    "    print('average score:', np.mean(scores))\n",
    "\n",
    "    plt.bar(range(len(scores)), scores)\n",
    "    plt.xlabel('folds')\n",
    "    plt.ylabel('score obtained')\n",
    "    plt.title('Cross validation result')\n",
    "    plt.hlines(np.mean(scores), xmin=0, xmax=9, linestyles='dashed', colors='black')\n",
    "    plt.legend(['mean', 'result CV'])\n",
    "    plt.show()\n",
    "\n",
    "    print(pd.DataFrame(zip(features, DT.feature_importances_),columns=['feature', 'importance']))\n",
    "    print('accuracy_training:', DT.score(X_train[features], y_train))\n",
    "    DT.fit(X, y)\n",
    "\n",
    "    return DT, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT, features_DT = run_decission_tree(set(feature_names)-set(['C1', 'C2', 'Fare', 'Age']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest(features):\n",
    "    \n",
    "    RFR = RandomForestClassifier(bootstrap=True, criterion='gini',\n",
    "                                max_features=0.25, min_samples_leaf=4,\n",
    "                                min_samples_split=3, n_estimators=100, random_state=0)\n",
    "\n",
    "    RFR.fit(X_train[features], y_train)\n",
    "\n",
    "    scores = cross_val_score(RFR, X_train, y_train, cv=10)\n",
    "    print('average score:', np.mean(scores))\n",
    "\n",
    "    plt.bar(range(len(scores)), scores)\n",
    "    plt.xlabel('folds')\n",
    "    plt.ylabel('score obtained')\n",
    "    plt.title('Cross validation result')\n",
    "    plt.hlines(np.mean(scores), xmin=0, xmax=9, linestyles='dashed', colors='black')\n",
    "    plt.legend(['mean', 'result CV'])\n",
    "    plt.show()\n",
    "\n",
    "    print(pd.DataFrame(zip(features, RFR.feature_importances_),columns=['feature', 'importance']))\n",
    "\n",
    "    print('accuracy_training:', RFR.score(X_train[features], y_train))\n",
    "    RFR.fit(X, y)\n",
    "    return RFR, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR, features_RFR = run_random_forest(set(feature_names)-set(['C1', 'C2',\n",
    "                                                              'Age', 'Fare', 'Isfemale',\n",
    "                                                              'CabinG', 'Isalone']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logit(features):\n",
    "    \n",
    "    logit = LogisticRegression(max_iter=1000)\n",
    "    logit.fit(X_train[features], y_train)\n",
    "    scores = cross_val_score(logit, X_train, y_train, cv=10)\n",
    "\n",
    "    print('average score:', np.mean(scores))\n",
    "\n",
    "    plt.bar(range(len(scores)), scores)\n",
    "    plt.xlabel('folds')\n",
    "    plt.ylabel('score obtained')\n",
    "    plt.title('Cross validation result')\n",
    "    plt.hlines(np.mean(scores), xmin=0, xmax=9, linestyles='dashed', colors='black')\n",
    "    plt.legend(['mean', 'result CV'])\n",
    "    plt.show()\n",
    "    \n",
    "    LR = Logit(y_train, X_train[features])\n",
    "    result = LR.fit()\n",
    "    print(result.summary2())\n",
    "    \n",
    "    print('accuracy_training:', logit.score(X_train[features], y_train))\n",
    "    logit.fit(X, y)\n",
    "    \n",
    "    return logit, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR, features_LR = run_logit(set(feature_names)-set(['C1', 'C2', 'Age', 'Fare',\n",
    "                                                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model selection <a class=\"anchor\" id=\"MS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, both models performed very similar, now, we will discuss some other alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDC = SGDClassifier(random_state=0, alpha=0.01, eta0=0.01, fit_intercept=True,\n",
    "             l1_ratio=0.5, learning_rate='constant', loss='hinge',\n",
    "             penalty='elasticnet', power_t=50.0)\n",
    "\n",
    "GDC.fit(X, y)\n",
    "scores = cross_val_score(GDC, X, y, cv=10)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRATREESCLASSIFICATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETC = ExtraTreesClassifier(random_state=0, bootstrap=True, criterion='entropy',\n",
    "                           max_features=0.35000000000000003, min_samples_leaf=12,\n",
    "                           min_samples_split=18, n_estimators=100)\n",
    "ETC_features = set(feature_names)-set(['C1', 'C2', 'Age', 'Fare'])\n",
    "ETC.fit(X[ETC_features], y)\n",
    "scores = cross_val_score(ETC, X[ETC_features], y, cv=10)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTILAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = MLPClassifier(alpha=0.001, learning_rate_init=0.001, max_iter=1000)\n",
    "MLP.fit(X_train, y_train)\n",
    "print(MLP.score(X_test, y_test))\n",
    "scores = cross_val_score(MLP, X, y, cv=10)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = XGBClassifier(learning_rate=0.01, max_depth=5,\n",
    "              min_child_weight=2, n_estimators=100, nthread=1, subsample=0.6500000000000001)\n",
    "XGB.fit(X, y)\n",
    "np.mean(cross_val_score(XGB, X, y, cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclussions <a class=\"anchor\" id=\"C\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "tpot = TPOTClassifier(generations=10, population_size=40, verbosity=1, scoring='accuracy', cv=cv)\n",
    "tpot_features = set(feature_names)-set(['C1', 'C2','Age', 'Fare', 'Isfemale',\n",
    "                                                              'CabinG', 'Isalone'])\n",
    "tpot.fit(X[tpot_features], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission <a class=\"anchor\" id=\"S\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the submission we will use the scripts, but the process is exactly the same as the above cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "fln.fill_age_values(test)\n",
    "fln.fill_embarked(test)\n",
    "fln.fill_fare_value(test)\n",
    "\n",
    "sub_features, _ = bf.build_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_result(sub_features, tpot, list(tpot_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
